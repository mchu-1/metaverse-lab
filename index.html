<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Network Lab</title>
    <meta name="description" content="Network Lab - 360 View" />
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <script src="https://aframe.io/releases/1.5.0/aframe.min.js"></script>
    <style>
      /* Ensure the scene takes up the full viewport */
      html,
      body {
        height: 100%;
        margin: 0;
        padding: 0;
        overflow: hidden;
        background-color: #000;
      }
    </style>
  </head>
  <body>
    <!-- 
      vr-mode-ui="enabled: true" enables the VR button.
      device-orientation-permission-ui handles iOS 13+ permission request automatically (in recent A-Frame versions).
    -->
    <a-scene vr-mode-ui="enabled: false" device-orientation-permission-ui="enabled: true">
      <a-assets>
        <img id="sky" src="nslab-world.png" />
      </a-assets>

      <!-- 360 Sky Sphere -->
      <a-sky src="#sky" rotation="0 -130 0"></a-sky>

      <!-- Camera with look controls -->
      <!-- reverseMouseDrag: true is often preferred for 360 drag interaction -->
      <a-entity
        camera
        look-controls="reverseMouseDrag: true; touchEnabled: true"
        position="0 1.6 0"
      ></a-entity>
    </a-scene>
    <div id="ui-controls">
      <button id="btn-fullscreen" class="ui-btn" aria-label="Toggle Fullscreen">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <path fill="currentColor" d="M7 14H5v5h5v-2H7v-3zm-2-4h2V7h3V5H5v5zm12 7h-3v2h5v-5h-2v3zM14 5v2h3v3h2V5h-5z"/>
        </svg>
      </button>
      <button id="btn-reset" class="ui-btn" aria-label="Reset View">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <path fill="currentColor" d="M12 5V1L7 6l5 5V7c3.31 0 6 2.69 6 6s-2.69 6-6 6-6-2.69-6-6H4c0 4.42 3.58 8 8 8s8-3.58 8-8-3.58-8-8-8z"/>
        </svg>
      </button>
      
      <!-- Connect Logic -->
      <button id="btn-connect" class="ui-btn" aria-label="Connect to Agent">
        <svg id="icon-mic-off" viewBox="0 0 24 24" width="24" height="24">
            <path fill="currentColor" d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.66 9 5v6c0 1.66 1.34 3 3 3z"/>
            <path fill="currentColor" d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/>
        </svg>
         <svg id="icon-mic-on" viewBox="0 0 24 24" width="24" height="24" style="display:none; color: #4CAF50;">
            <path fill="currentColor" d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.66 9 5v6c0 1.66 1.34 3 3 3z"/>
            <path fill="currentColor" d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/>
            <circle cx="18" cy="5" r="3" fill="#4CAF50"/>
        </svg>
      </button>
    </div>

    <!-- Debug Console Overlay -->
    <div id="debug-console"></div>

    <style>
      #debug-console {
        position: absolute;
        top: 10px;
        left: 10px;
        width: 300px;
        max-height: 200px;
        overflow-y: auto;
        background: rgba(0, 0, 0, 0.7);
        color: #0f0;
        font-family: monospace;
        font-size: 10px;
        padding: 5px;
        pointer-events: none; /* Let touches pass through */
        z-index: 9999;
        display: block; /* Visible by default for this test */
        border: 1px solid #333;
      }
      .log-entry { margin-bottom: 2px; border-bottom: 1px solid #222; }
      .log-info { color: #88ff88; }
      .log-warn { color: #ffff88; }
      .log-error { color: #ff8888; }
    </style>

    <script>
      // ON-SCREEN LOGGER
      (function() {
        const consoleDiv = document.getElementById('debug-console');
        const MAX_LOGS = 50;
        
        function sendLog(level, args) {
            try {
                const msg = args.map(a => (typeof a === 'object' ? JSON.stringify(a) : String(a))).join(' ');
                fetch('/log', { method: 'POST', body: `[${level}] ${msg}` }).catch(e => {});
            } catch (e) {}
        }
        
        function uiLog(level, args) {
             const msg = args.map(a => (typeof a === 'object' ? JSON.stringify(a) : String(a))).join(' ');
             
             // 1. Update UI
             const line = document.createElement('div');
             line.className = `log-entry log-${level.toLowerCase()}`;
             line.innerText = `[${new Date().toLocaleTimeString().split(' ')[0]}] ${msg}`;
             consoleDiv.appendChild(line);
             consoleDiv.scrollTop = consoleDiv.scrollHeight;
             if (consoleDiv.children.length > MAX_LOGS) consoleDiv.removeChild(consoleDiv.firstChild);
             
             // 2. Send Remote (Best Effort)
             sendLog(level, args);
        }

        const originalLog = console.log;
        const originalWarn = console.warn;
        const originalError = console.error;

        console.log = function(...args) { originalLog.apply(console, args); uiLog('INFO', args); };
        console.warn = function(...args) { originalWarn.apply(console, args); uiLog('WARN', args); };
        console.error = function(...args) { originalError.apply(console, args); uiLog('ERROR', args); };
      })();
    </script>
    <script type="module">
      import { LiveClient } from './live-client.js';
      import { AudioRecorder, AudioStreamPlayer } from './audio-utils.js';

      // UI Logic
      const btnFullscreen = document.getElementById('btn-fullscreen');
      const btnReset = document.getElementById('btn-reset');
      const btnConnect = document.getElementById('btn-connect');
      const iconMicOff = document.getElementById('icon-mic-off');
      const iconMicOn = document.getElementById('icon-mic-on');
      
      const scene = document.querySelector('a-scene');
      const cameraEntity = document.querySelector('[camera]');

      // State
      let isConnected = false;
      let liveClient = null;
      let audioRecorder = null;
      let audioPlayer = null;
      let speechRecognition = null;
      
      // Data (exposed for the Gaze logic)
      let annotations = [];
      let lastLookedAt = null;
      let lastAnnounceTime = 0;

      // --- 1. Basic UI Handlers ---

      btnFullscreen.addEventListener('click', () => {
        if (!document.fullscreenElement) {
          document.documentElement.requestFullscreen();
        } else {
          if (document.exitFullscreen) document.exitFullscreen();
        }
      });

      btnReset.addEventListener('click', () => {
        if (cameraEntity) {
           cameraEntity.setAttribute('rotation', '0 0 0');
           const controls = cameraEntity.components['look-controls'];
           if (controls) {
             controls.pitchObject.rotation.x = 0;
             controls.yawObject.rotation.y = 0;
           }
        }
      });

      // --- 2. Live Agent Logic ---

      btnConnect.addEventListener('click', async () => {
        if (isConnected) {
            disconnectAgent();
        } else {
            await connectAgent();
        }
      });

      async function connectAgent() {
        let apiKey = null;
        
        // 1. Try to load from env.js (Environment Priority)
        try {
            const env = await import('./env.js');
            if (env.GEMINI_API_KEY) {
                apiKey = env.GEMINI_API_KEY;
                console.log("Using API Key from env.js");
            }
        } catch (e) {
            console.log("env.js not found (Production or Missing).");
        }

        // 2. Try LocalStorage (Fallback)
        if (!apiKey) {
            apiKey = localStorage.getItem('GEMINI_API_KEY');
        }

        // 3. Prompt (Last Resort)
        if (!apiKey) {
             apiKey = prompt("Enter Google Gemini API Key:");
        }
        
        if (!apiKey) return;
        localStorage.setItem('GEMINI_API_KEY', apiKey); // Cache it

        btnConnect.style.opacity = '0.5'; // Loading state

        try {
            liveClient = new LiveClient(apiKey);
            audioRecorder = new AudioRecorder();
            audioPlayer = new AudioStreamPlayer();

            // 1. Initialize Audio Output
            await audioPlayer.initialize();

            // 2. Connect generic callback
            liveClient.onAudio = (data) => {
                audioPlayer.addPCM16(data);
            };

            liveClient.onClose = () => {
                disconnectAgent();
            };

            // Handle Interruption Signal
            window.addEventListener('gemini-interrupted', () => {
                if (audioPlayer) audioPlayer.clear();
            });

            // 3. Build System Instruction with Context
            const reason = document.getElementById('reason-content').innerText;
            const equip = document.getElementById('equipment-list').innerText;
            const legal = document.getElementById('legal-list').innerText;
            const annotations = document.getElementById('annotations-list').innerText;

            // We construct a prompt that prepares the agent
            const instructions = `
You are an intelligent AI lab assistant in the 'Network Lab'. 
You have access to the user's focus of attention via system messages I will send you.
When I send "CONTEXT: User is looking at [Object Name]", you should be aware of it.
If the object is interesting or the user dwells on it, you can offer a brief, interesting fact or description.
Don't be too chatty. Be helpful, concise, and professional.

Context Data:
${reason}

Equipment List:
${equip}

Legal Requirements:
${legal}

Known Annotations/Locations:
${annotations}
            `;

            // 4. Connect WebSocket
            await liveClient.connect(instructions);

            // 5. Start Microphone
            await audioRecorder.start((base64PCM) => {
                liveClient.sendAudio(base64PCM);
            });

            // Success Updates
            isConnected = true;
            btnConnect.style.opacity = '1';
            iconMicOff.style.display = 'none';
            iconMicOn.style.display = 'block';
            
            
            // Start Gaze Loop
            startGazeTracking();
            
            // Start Local Speech Logging
            startSpeechLogger();

        } catch (e) {
            console.error("Connection failed", e);
            alert("Failed to connect: " + e.message);
            disconnectAgent();
        }
      }

      function disconnectAgent() {
        isConnected = false;
        if (liveClient) liveClient.disconnect();
        if (audioRecorder) audioRecorder.stop();
        
        liveClient = null;
        audioRecorder = null;
        audioPlayer = null;

        btnConnect.style.opacity = '1';
        iconMicOff.style.display = 'block';
        iconMicOn.style.display = 'none';
        
        stopGazeTracking();
        stopSpeechLogger();
      }

      // --- 2.5 Local Speech Logging (Web Speech API) ---
      function startSpeechLogger() {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!SpeechRecognition) {
            console.warn("Speech Recognition API not supported in this browser.");
            return;
        }
        
        // Prevent duplicates
        if (speechRecognition) stopSpeechLogger();

        speechRecognition = new SpeechRecognition();
        speechRecognition.continuous = true;
        speechRecognition.interimResults = false;
        speechRecognition.lang = 'en-US';

        speechRecognition.onresult = (event) => {
            if (event.results.length > 0) {
               const transcript = event.results[event.results.length - 1][0].transcript;
               if (transcript.trim()) {
                 console.log("[USER]", transcript);
               }
            }
        };
        
        speechRecognition.onend = () => {
            // Auto-restart if we are still connected
            if (isConnected && speechRecognition) {
                 try { speechRecognition.start(); } catch(e){}
            }
        };

        try {
            speechRecognition.start();
        } catch(e) {
            console.error("Failed to start speech logging:", e);
        }
      }

      function stopSpeechLogger() {
        if (speechRecognition) {
            const recognition = speechRecognition;
            speechRecognition = null; // Unset to prevent auto-restart in onend
            try { recognition.stop(); } catch(e) {}
        }
      }

      // --- 3. Gaze Tracking Logic ---
      
      let gazeInterval = null;
      let dwellTimer = null;
      const DWELL_THRESHOLD = 1500; // 1.5s to trigger
      const COOLDOWN = 20000; // 20s before mentioning same thing again
      const history = {}; // { "Object": timestamp }

      function startGazeTracking() {
          if (gazeInterval) clearInterval(gazeInterval);
          gazeInterval = setInterval(checkGaze, 200); // Check 5 times a second
      }

      function stopGazeTracking() {
          if (gazeInterval) clearInterval(gazeInterval);
          if (dwellTimer) clearTimeout(dwellTimer);
          gazeInterval = null;
      }

      function checkGaze() {
          if (!cameraEntity) return;
          
          const rotation = cameraEntity.object3D.rotation;
          // Three.js rotation is in Radians. Order usually YXZ.
          // A-Frame: Y is Yaw (Vertical Axis), X is Pitch (Horizontal axis)
          
          let theta = rotation.y; // Yaw
          let phi = rotation.x;   // Pitch
          
          // Normalize Yaw to 0..2PI
          // In A-Frame/Three, positive rotation is counter-clockwise usually.
          // To map to equirectangular:
          // u = 0.5 + atan2(z, x) / 2pi ... simpler approach using degrees helpers if available
          // Let's use simple degree mapping based on experience with A-Frame Sky
          
          // Convert to degrees
          const rad2deg = 180 / Math.PI;
          let yawDeg = (theta * rad2deg) % 360;
          let pitchDeg = (phi * rad2deg);
          
          // Normalize to [0, 360)
          if (yawDeg < 0) yawDeg += 360;
          
          // Map to Percentage (0..100)
          // Note: The sky texture is rotated "rotation='0 -130 0'". 
          // We need to account for this offset.
          // The CSS/Canvas coordinate system usually starts Top-Left.
          // A-Frame yaw 0 is usually -Z.
          
          // We'll use a simplified heuristic that matches the existing annotation system 
          // (which likely uses a simple grid).
          // We just need to tune the Offset. The image is rotated -130 deg.
          
          // Invert yaw because dragging left moves camera positive rot but texture moves right?
          // Let's try standard mapping:
          // U = (Yaw + Offset) / 360
          // V = (Pitch + 90) / 180
          
          // Tweaked for 'rotation="0 -130 0"'
          let u = ((360 - yawDeg) - 130) % 360; 
          if (u < 0) u += 360;
          u = u / 360;

          // Pitch: Up is positive in older A-Frame? No, usually Looking Up is Positive X in local, 
          // but verify A-Frame camera. look-controls usually: Pitch down = negative X?
          // Actually, simply:
          // -90 (down) to +90 (up) -> 0 (top) to 1 (bottom) in UV?
          // Typically texture V=0 is bottom, V=1 is top in GL, but HTML Canvas 0 is top.
          // Let's assume annotations.csv Y uses "Top is 0%" (HTML style).
          // Then Pitch +90 -> 0 is Bottom? 
          
          // Let's rely on Normalized Coords derived from the csv:
          // The CSV likely has X/Y in pixels relative to 1024x506.
          
          let v = 0.5 - (Math.max(-90, Math.min(90, pitchDeg)) / 180); 
          // if pitch is 0 (horizon), v is 0.5.
          // if pitch is 90 (up), item is at top? Wait. 
          // If I look up, I see the sky (Top of image).
          // If standard texture mapping: V=1 is Top.
          // If CSV uses HTML coords: Y=0 is Top.
          // So if Look Up (+90), we want Y=0.
          // v = 0.5 - (90/180) = 0 -> Correct.
          
          const hit = findAnnotation(u, v);
          
          if (hit) {
              handleObjectInView(hit);
          } else {
             // Lost focus
             lastLookedAt = null;
             if (dwellTimer) {
                 clearTimeout(dwellTimer);
                 dwellTimer = null;
             }
          }
      }

      function findAnnotation(u, v) {
          // u, v are 0..1
          // annotations is global populated below
          if (!window.labAnnotations) return null;
          
          // x, y, width, height in CSV are pixels relative to 1024x506
          const W = 1024;
          const H = 506;
          
          for (const item of window.labAnnotations) {
              const annX = parseFloat(item.x) / W;
              const annY = parseFloat(item.y) / H;
              const annW = parseFloat(item.width) / W;
              const annH = parseFloat(item.height) / H;
              
              if (u >= annX && u <= annX + annW &&
                  v >= annY && v <= annY + annH) {
                  return item.annotation;
              }
          }
          return null;
      }
      
      function handleObjectInView(objectName) {
          if (lastLookedAt === objectName) return; // Already tracking this dwell
          
          lastLookedAt = objectName;
          
          // Reset Dwell
          if (dwellTimer) clearTimeout(dwellTimer);
          
          // Start Dwell
          dwellTimer = setTimeout(() => {
              triggerContextUpdate(objectName);
          }, DWELL_THRESHOLD);
      }
      
      function triggerContextUpdate(objectName) {
          const now = Date.now();
          // Cooldown check
          if (history[objectName] && (now - history[objectName] < COOLDOWN)) {
              return; // Too soon
          }
          
          console.log("Triggering Context:", objectName);
          history[objectName] = now;
          
          // Send to Gemini
          if (isConnected && liveClient) {
              liveClient.sendTextContext(`CONTEXT: User is looking at object: "${objectName}".`);
          }
      }

    </script>
    <style>
      #ui-controls {
        position: absolute;
        bottom: 20px;
        right: 20px;
        z-index: 1000;
        display: flex;
        gap: 10px;
      }
      
      .ui-btn {
        background: rgba(20, 20, 20, 0.6);
        border: 1px solid rgba(255, 255, 255, 0.2);
        border-radius: 8px;
        color: white;
        padding: 10px;
        cursor: pointer;
        backdrop-filter: blur(4px);
        transition: all 0.2s ease;
        display: flex;
        align-items: center;
        justify-content: center;
      }

      .ui-btn:hover {
        background: rgba(20, 20, 20, 0.8);
        transform: translateY(-2px);
      }
      
      .ui-btn:active {
        transform: translateY(0);
      }

      /* Accessibility Tree (Visually Hidden) */
      .visually-hidden {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
      }


    </style>

    <!-- Hidden Accessibility Layer for AI Agents -->
    <div id="accessibility-tree" class="visually-hidden" aria-hidden="false">
      <section id="env-reason" aria-labelledby="reason-title">
        <h1 id="reason-title">Environment Purpose</h1>
        <div id="reason-content"></div>
      </section>
      <section id="env-equipment" aria-labelledby="equipment-title">
        <h2 id="equipment-title">Laboratory Equipment</h2>
        <div id="equipment-list"></div>
      </section>
      <section id="env-annotations" aria-labelledby="annotations-title">
        <h2 id="annotations-title">Annotated Objects</h2>
        <div id="annotations-list"></div>
      </section>
      <section id="env-legal" aria-labelledby="legal-title">
        <h2 id="legal-title">Legal & Compliance</h2>
        <div id="legal-list"></div>
      </section>
    </div>

    <script>
      // Source resolution
      const SOURCE_WIDTH = 1024;
      const SOURCE_HEIGHT = 506; 

      // Load and inject metadata
      async function loadMetadata() {
        try {
          // Load Reason
          const reasonRes = await fetch('reason.txt');
          if (reasonRes.ok) {
            const reasonText = await reasonRes.text();
            document.getElementById('reason-content').innerText = reasonText;
          }

          // Load Equipment
          const equipRes = await fetch('equipment.csv');
          if (equipRes.ok) {
            const csvText = await equipRes.text();
            const equipmentData = parseCSV(csvText);
            renderTable(equipmentData, 'equipment-list');
          }

          // Load Legal
          const legalRes = await fetch('legal.csv');
          if (legalRes.ok) {
            const legalText = await legalRes.text();
            const legalData = parseCSV(legalText);
            renderTable(legalData, 'legal-list');
          }

          // Load Annotations
          const annRes = await fetch('annotations.csv');
          if (annRes.ok) {
            const annText = await annRes.text();
            const annData = parseCSV(annText);
            renderAnnotations(annData, 'annotations-list');
            window.labAnnotations = annData; // Expose for Gaze Tracker
          }

        } catch (e) {
          console.error("Failed to load metadata:", e);
        }
      }

      function parseCSV(text) {
          // ... (existing helper) ...
        const lines = text.trim().split('\n');
        if (lines.length < 2) return []; // Access headers safely
        const headers = parseLine(lines[0]);
        
        return lines.slice(1).map(line => {
          const values = parseLine(line);
          return headers.reduce((obj, header, index) => {
            obj[header.trim()] = values[index];
            return obj;
          }, {});
        });
      }
      
      // ... (parseLine helper) ...
       function parseLine(line) {
        const result = [];
        let current = '';
        let inQuotes = false;
        
        for (let i = 0; i < line.length; i++) {
          const char = line[i];
          if (char === '"') {
            inQuotes = !inQuotes;
          } else if (char === ',' && !inQuotes) {
            result.push(current.trim());
            current = '';
          } else {
            current += char;
          }
        }
        result.push(current.trim());
        return result;
      }
      
      // ... (renderTable helper) ...
      function renderTable(data, containerId) {
        const container = document.getElementById(containerId);
        if (!container) return;
        
        const table = document.createElement('table');
        const thead = document.createElement('thead');
        const tbody = document.createElement('tbody');

        if (!data || data.length === 0) return;

        // Headers
        const headers = Object.keys(data[0]);
        const trHead = document.createElement('tr');
        headers.forEach(h => {
          const th = document.createElement('th');
          th.innerText = h;
          trHead.appendChild(th);
        });
        thead.appendChild(trHead);

        // Rows
        data.forEach(item => {
          const tr = document.createElement('tr');
          headers.forEach(h => {
            const td = document.createElement('td');
            td.innerText = item[h] || '';
            tr.appendChild(td);
          });
          tbody.appendChild(tr);
        });

        table.appendChild(thead);
        table.appendChild(tbody);
        container.appendChild(table);
      }

      function renderAnnotations(data, containerId) {
        const container = document.getElementById(containerId);
        if (!container || !data || data.length === 0) return;

        const uList = document.createElement('ul');
        
        data.forEach(item => {
          const x = parseFloat(item.x);
          const y = parseFloat(item.y);
          const w = parseFloat(item.width);
          const h = parseFloat(item.height);
          
          if (isNaN(x) || isNaN(y)) return;

          // Normalize to percentage for description
          const top = (y / SOURCE_HEIGHT * 100).toFixed(2);
          const left = (x / SOURCE_WIDTH * 100).toFixed(2);
          const width = (w / SOURCE_WIDTH * 100).toFixed(2);
          const height = (h / SOURCE_HEIGHT * 100).toFixed(2);

          const li = document.createElement('li');
          // Text description for AI agent
          li.innerText = `${item.annotation}: located at position top=${top}%, left=${left}%, width=${width}%, height=${height}% of the source view.`;
          uList.appendChild(li);
        });

        container.appendChild(uList);
      }

      // Initialize
      loadMetadata();
    </script>
  </body>
</html>
