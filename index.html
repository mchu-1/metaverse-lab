<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Network Lab</title>
    <meta name="description" content="Network Lab - 360 View" />
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <script src="https://aframe.io/releases/1.5.0/aframe.min.js"></script>
    <style>
      /* Ensure the scene takes up the full viewport */
      html,
      body {
        height: 100%;
        margin: 0;
        padding: 0;
        overflow: hidden;
        background-color: #000;
      }
    </style>
  </head>
  <body>
    <!-- 
      vr-mode-ui="enabled: true" enables the VR button.
      device-orientation-permission-ui handles iOS 13+ permission request automatically (in recent A-Frame versions).
    -->
    <a-scene vr-mode-ui="enabled: false" device-orientation-permission-ui="enabled: true" renderer="colorManagement: true; preserveDrawingBuffer: true;">
      <a-assets>
        <img id="sky" src="nslab-world.png" />
      </a-assets>

      <!-- 360 Sky Sphere -->
      <a-sky src="#sky" rotation="0 -130 0"></a-sky>

      <!-- Camera with look controls -->
      <!-- reverseMouseDrag: true is often preferred for 360 drag interaction -->
      <a-entity
        camera
        look-controls="reverseMouseDrag: true; touchEnabled: true"
        position="0 1.6 0"
      ></a-entity>
    </a-scene>
    <div id="ui-controls">
      <button id="btn-fullscreen" class="ui-btn" aria-label="Toggle Fullscreen">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <path fill="currentColor" d="M7 14H5v5h5v-2H7v-3zm-2-4h2V7h3V5H5v5zm12 7h-3v2h5v-5h-2v3zM14 5v2h3v3h2V5h-5z"/>
        </svg>
      </button>
      <button id="btn-reset" class="ui-btn" aria-label="Reset View">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <path fill="currentColor" d="M12 5V1L7 6l5 5V7c3.31 0 6 2.69 6 6s-2.69 6-6 6-6-2.69-6-6H4c0 4.42 3.58 8 8 8s8-3.58 8-8-3.58-8-8-8z"/>
        </svg>
      </button>
      
      <!-- Connect Logic -->
      <button id="btn-connect" class="ui-btn" aria-label="Connect to Agent">
        <svg id="icon-mic-off" viewBox="0 0 24 24" width="24" height="24">
            <path fill="currentColor" d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.66 9 5v6c0 1.66 1.34 3 3 3z"/>
            <path fill="currentColor" d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/>
        </svg>
         <svg id="icon-mic-on" viewBox="0 0 24 24" width="24" height="24" style="display:none; color: #4CAF50;">
            <path fill="currentColor" d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.66 9 5v6c0 1.66 1.34 3 3 3z"/>
            <path fill="currentColor" d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/>
            <circle cx="18" cy="5" r="3" fill="#4CAF50"/>
        </svg>
      </button>
    </div>

    <!-- Debug Console Overlay -->




    <script>
      // REMOTE LOGGER (UI Removed)
      (function() {
        // No visual console
        
        function sendLog(level, args) {
            try {
                const msg = args.map(a => (typeof a === 'object' ? JSON.stringify(a) : String(a))).join(' ');
                fetch('/log', { method: 'POST', body: `[${level}] ${msg}` }).catch(e => {});
            } catch (e) {}
        }
        
        function uiLog(level, args) {
             // 1. UI Update Removed
             
             // 2. Send Remote (Best Effort)
             sendLog(level, args);
        }

        const originalLog = console.log;
        const originalWarn = console.warn;
        const originalError = console.error;

        console.log = function(...args) { originalLog.apply(console, args); uiLog('INFO', args); };
        console.warn = function(...args) { originalWarn.apply(console, args); uiLog('WARN', args); };
        console.error = function(...args) { originalError.apply(console, args); uiLog('ERROR', args); };
      })();
    </script>
    <script type="module">
      import { LiveClient } from './live-client.js';
      import { AudioRecorder, AudioStreamPlayer } from './audio-utils.js';

      // UI Logic
      const btnFullscreen = document.getElementById('btn-fullscreen');
      const btnReset = document.getElementById('btn-reset');
      const btnConnect = document.getElementById('btn-connect');
      const iconMicOff = document.getElementById('icon-mic-off');
      const iconMicOn = document.getElementById('icon-mic-on');
      
      const scene = document.querySelector('a-scene');
      const cameraEntity = document.querySelector('[camera]');

      // State
      let isConnected = false;
      let liveClient = null;
      let audioRecorder = null;
      let audioPlayer = null;
      let speechRecognition = null;
      
      // Data (exposed for the Gaze logic)
      let annotations = [];
      let lastLookedAt = null;
      let lastAnnounceTime = 0;

      // EXPOSE FOR DEBUGGING
      window.labTest = {
          getLiveClient: () => liveClient,
          startSpeechLogger: () => startSpeechLogger(),
          stopSpeechLogger: () => stopSpeechLogger(),
          setLiveClient: (client) => { liveClient = client; },
          setIsConnected: (val) => { isConnected = val; }
      };

      // --- 1. Basic UI Handlers ---

      // Global Control Interface for AI Agent
      // Global Control Interface for AI Agent
      window.labControl = {
          animationRef: null,
          
          reset: () => {
             window.labControl.cancelAnimation();
             if (cameraEntity) {
               cameraEntity.setAttribute('rotation', '0 0 0');
               const controls = cameraEntity.components['look-controls'];
               if (controls) {
                 controls.pitchObject.rotation.x = 0;
                 controls.yawObject.rotation.y = 0;
               }
             }
          },

          cancelAnimation: () => {
              if (window.labControl.animationRef) {
                  cancelAnimationFrame(window.labControl.animationRef);
                  window.labControl.animationRef = null;
              }
          },
          
          setRotation: (targetYawDeg, targetPitchDeg, duration = 1500) => {
             window.labControl.cancelAnimation();
             
             if (!cameraEntity) return;

             // A-Frame Look Controls overrides rotation directly
             const controls = cameraEntity.components['look-controls'];
             if (!controls) {
                 // Fallback if no controls
                 cameraEntity.setAttribute('rotation', `${targetPitchDeg} ${targetYawDeg} 0`);
                 return;
             }

             // current rotations in radians
             const startYawRad = controls.yawObject.rotation.y;
             const startPitchRad = controls.pitchObject.rotation.x;
             
             const startYawDeg = THREE.MathUtils.radToDeg(startYawRad);
             const startPitchDeg = THREE.MathUtils.radToDeg(startPitchRad);

             // Calculate shortest path for Yaw
             let deltaYaw = (targetYawDeg - startYawDeg) % 360;
             if (deltaYaw > 180) deltaYaw -= 360;
             if (deltaYaw < -180) deltaYaw += 360;
             
             const endYawRad = THREE.MathUtils.degToRad(startYawDeg + deltaYaw);
             const endPitchRad = THREE.MathUtils.degToRad(targetPitchDeg);

             const startTime = performance.now();
             
             function animate(time) {
                 const elapsed = time - startTime;
                 const progress = Math.min(elapsed / duration, 1.0);
                 
                 // Ease In Out Cubic
                 const ease = progress < 0.5 
                    ? 4 * progress * progress * progress 
                    : 1 - Math.pow(-2 * progress + 2, 3) / 2;

                 const currentYaw = startYawRad + (endYawRad - startYawRad) * ease;
                 const currentPitch = startPitchRad + (endPitchRad - startPitchRad) * ease;

                 controls.yawObject.rotation.y = currentYaw;
                 controls.pitchObject.rotation.x = currentPitch;

                 if (progress < 1.0) {
                     window.labControl.animationRef = requestAnimationFrame(animate);
                 } else {
                     window.labControl.animationRef = null;
                 }
             }
             
             window.labControl.animationRef = requestAnimationFrame(animate);
          }
      };

      btnFullscreen.addEventListener('click', () => {
        if (!document.fullscreenElement) {
          document.documentElement.requestFullscreen();
        } else {
          if (document.exitFullscreen) document.exitFullscreen();
        }
      });

      btnReset.addEventListener('click', () => {
         window.labControl.reset();
      });

      // --- 2. Live Agent Logic ---

      btnConnect.addEventListener('click', async () => {
        if (isConnected) {
            disconnectAgent();
        } else {
            await connectAgent();
        }
      });

      async function connectAgent() {
        let apiKey = null;
        
            // 1. Fetch Ephemeral Token from Backend
            try {
                const tokenRes = await fetch('/token');
                if (!tokenRes.ok) throw new Error("Failed to fetch ephemeral token");
                const tokenData = await tokenRes.json();
                apiKey = tokenData.token;
                console.log("Using Ephemeral Token");
            } catch (e) {
                console.error("Token fetch failed, falling back to cached/prompt:", e);
            }

            // 2. Fallback: Try LocalStorage / Prompt if token fetch failed (or if you want to keep legacy support)
            if (!apiKey) {
                const stored = localStorage.getItem('GEMINI_API_KEY');
                if (stored && typeof stored === 'string' && stored.trim().length > 0) {
                    apiKey = stored.trim();
                }
            }

            if (!apiKey) {
                 const input = prompt("Enter Google Gemini API Key (or ensure server is running):");
                 if (input && input.trim().length > 0) {
                    apiKey = input.trim();
                 }
            }
            
            if (!apiKey) return;
            // Note: We don't cache ephemeral tokens in localStorage as they expire
            if (!apiKey.startsWith('AIza')) { 
                 // It's likely an ephemeral token or we just rely on it. 
                 // If it was a real key entered manually, we might want to cache it.
                 // For now, let's only cache if it looks like a standard key (optional)
            } else {
                 localStorage.setItem('GEMINI_API_KEY', apiKey);
            }

            btnConnect.style.opacity = '0.5'; // Loading state

            try {
                liveClient = new LiveClient(apiKey);
            audioRecorder = new AudioRecorder();
            audioPlayer = new AudioStreamPlayer();

            // 1. Initialize Audio Output
            await audioPlayer.initialize();

            // 2. Connect generic callback
            liveClient.onAudio = (data) => {
                audioPlayer.addPCM16(data);
            };

            liveClient.onClose = () => {
                disconnectAgent();
            };

            // Handle Interruption Signal
            window.addEventListener('gemini-interrupted', () => {
                if (audioPlayer) audioPlayer.clear();
            });

            // 3. Build System Instruction with Context
            const reason = document.getElementById('reason-content').innerText;
            const equip = document.getElementById('equipment-list').innerText;
            const legal = document.getElementById('legal-list').innerText;
            const annotations = document.getElementById('annotations-list').innerText;

            // We construct a prompt that prepares the agent
            const instructions = `
You are an intelligent AI lab assistant in the 'Network Lab'. 
You have access to the user's focus of attention via system messages I will send you.
You can also use the get_visual_context tool to take a photo of what the user is seeing and get a description. Use this when the user asks "What do I see?" or "Describe this".
When I send "CONTEXT: User is looking at [Object Name]", you should be aware of it.
If the object is interesting or the user dwells on it, you can offer a brief, interesting fact or description.
Don't be too chatty. Be helpful, concise, and professional.

Context Data:
${reason}

Equipment List:
${equip}

Legal Requirements:
${legal}

Known Annotations/Locations:
${annotations}
            `;

            // 4. Connect WebSocket
            await liveClient.connect(instructions);

            // 5. Start Microphone
            await audioRecorder.start((base64PCM) => {
                liveClient.sendAudio(base64PCM);
            });

            // Success Updates
            isConnected = true;
            btnConnect.style.opacity = '1';
            iconMicOff.style.display = 'none';
            iconMicOn.style.display = 'block';
            
            
            // Start Gaze Loop
            startGazeTracking();
            
            // Start Local Speech Logging
            startSpeechLogger();

        } catch (e) {
            console.error("Connection failed", e);
            alert("Failed to connect: " + e.message);
            disconnectAgent();
        }
      }

      function disconnectAgent() {
        isConnected = false;
        if (liveClient) liveClient.disconnect();
        if (audioRecorder) audioRecorder.stop();
        
        liveClient = null;
        audioRecorder = null;
        audioPlayer = null;

        btnConnect.style.opacity = '1';
        iconMicOff.style.display = 'block';
        iconMicOn.style.display = 'none';
        
        stopGazeTracking();
        stopSpeechLogger();
      }

      // --- 2.5 Local Speech Logging (Web Speech API) ---
      function startSpeechLogger() {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!SpeechRecognition) {
            console.warn("Speech Recognition API not supported in this browser.");
            return;
        }
        
        // Prevent duplicates
        if (speechRecognition) stopSpeechLogger();

        speechRecognition = new SpeechRecognition();
        speechRecognition.continuous = true;
        speechRecognition.interimResults = false;
        speechRecognition.lang = 'en-US';

        speechRecognition.onresult = (event) => {
            if (event.results.length > 0) {
               const transcript = event.results[event.results.length - 1][0].transcript;
               if (transcript.trim()) {
                 console.log("[USER]", transcript);
                 
                 // Forward to Agent as Context
                 // REMOVED: Redundant with Audio Streaming (Speech-to-Speech)
                 // if (isConnected && liveClient) {
                 //     liveClient.sendTextContext(`CONTEXT: User said: "${transcript}"`);
                 // }
               }
            }
        };
        
        speechRecognition.onend = () => {
            // Auto-restart if we are still connected
            if (isConnected && speechRecognition) {
                 try { speechRecognition.start(); } catch(e){}
            }
        };

        try {
            speechRecognition.start();
        } catch(e) {
            console.error("Failed to start speech logging:", e);
        }
      }

      function stopSpeechLogger() {
        if (speechRecognition) {
            const recognition = speechRecognition;
            speechRecognition = null; // Unset to prevent auto-restart in onend
            try { recognition.stop(); } catch(e) {}
        }
      }

      // --- 3. Gaze Tracking Logic (Enhanced for FOV) ---
      
      let gazeInterval = null;
      let lastVisibleObjects = ""; // String signature of confirmed visible objects
      
      // Dwell Logic
      let candidateVisible = "";
      let candidateStartTime = 0;
      const DWELL_MS = 2000; // 2 seconds dwell required

      // FOV Configuration
      const FOV_H_DEG = 80; 
      const FOV_V_DEG = 60;

      function startGazeTracking() {
          if (gazeInterval) clearInterval(gazeInterval);
          gazeInterval = setInterval(checkView, 500); // Check every 500ms for better responsiveness
      }

      function stopGazeTracking() {
          if (gazeInterval) clearInterval(gazeInterval);
          gazeInterval = null;
      }

      function checkView() {
          if (!cameraEntity || !window.labAnnotations) return;
          
          const rotation = cameraEntity.object3D.rotation;
          let theta = rotation.y; // Yaw
          let phi = rotation.x;   // Pitch
          
          const rad2deg = 180 / Math.PI;
          let yawDeg = (theta * rad2deg) % 360;
          let pitchDeg = (phi * rad2deg);
          
          if (yawDeg < 0) yawDeg += 360;
          
          // Current Center UV (Similar to before)
          // Adjust for rotation '0 -130 0'
          let centerU = ((360 - yawDeg) - 130) % 360; 
          if (centerU < 0) centerU += 360;
          centerU = centerU / 360;
          
          let centerV = 0.5 - (Math.max(-90, Math.min(90, pitchDeg)) / 180);
          
          // Calculate Visible Annotations
          // We'll define a simple box in UV space around centerU, centerV
          // Note: This wraps around U (0-1).
          const halfH = (FOV_H_DEG / 2) / 360; 
          const halfV = (FOV_V_DEG / 2) / 180;
          
          const visible = [];
          
          for (const item of window.labAnnotations) {
               // Item Center
               const W = 1024;
               const H = 506;
               const annX = parseFloat(item.x) / W;
               const annY = parseFloat(item.y) / H;
               const annW = parseFloat(item.width) / W;
               const annH = parseFloat(item.height) / H;
               
               const itemCX = annX + (annW/2);
               const itemCY = annY + (annH/2);
               
               // Check Range V
               if (Math.abs(itemCY - centerV) > halfV + (annH/2)) continue;
               
               // Check Range U (Handle Wrapping)
               let diffU = Math.abs(itemCX - centerU);
               if (diffU > 0.5) diffU = 1.0 - diffU; // Shortest path around cylinder
               
               if (diffU <= halfH + (annW/2)) {
                   visible.push(item.annotation);
               }
          }
          
          // Sort for stability
          visible.sort();
          const signature = visible.join(", ");

          // DWELL LOGIC STATE MACHINE
          if (signature !== candidateVisible) {
              // CHANGE DETECTED: Start timer for new candidate
              // console.log("Gaze candidate changed:", signature);
              candidateVisible = signature;
              candidateStartTime = Date.now();
          } else {
              // NO CHANGE: Check if we have dwelled long enough
              const elapsed = Date.now() - candidateStartTime;
              
              if (elapsed >= DWELL_MS && signature !== lastVisibleObjects) {
                   // CONFIRMED!
                   lastVisibleObjects = signature;
                   console.log("Confirmed Gaze Focus:", signature);

                   if (isConnected && liveClient && signature) {
                        liveClient.sendTextContext(`CONTEXT: User is looking at: [${signature}]`);
                   }
              }
          }
      }
    </script>
    <style>
      #ui-controls {
        position: absolute;
        bottom: 20px;
        right: 20px;
        z-index: 1000;
        display: flex;
        gap: 10px;
      }
      
      .ui-btn {
        background: rgba(20, 20, 20, 0.6);
        border: 1px solid rgba(255, 255, 255, 0.2);
        border-radius: 8px;
        color: white;
        padding: 10px;
        cursor: pointer;
        backdrop-filter: blur(4px);
        transition: all 0.2s ease;
        display: flex;
        align-items: center;
        justify-content: center;
      }

      .ui-btn:hover {
        background: rgba(20, 20, 20, 0.8);
        transform: translateY(-2px);
      }
      
      .ui-btn:active {
        transform: translateY(0);
      }

      /* Accessibility Tree (Visually Hidden) */
      .visually-hidden {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
      }


    </style>

    <!-- Hidden Accessibility Layer for AI Agents -->
    <div id="accessibility-tree" class="visually-hidden" aria-hidden="false">
      <section id="env-reason" aria-labelledby="reason-title">
        <h1 id="reason-title">Environment Purpose</h1>
        <div id="reason-content"></div>
      </section>
      <section id="env-equipment" aria-labelledby="equipment-title">
        <h2 id="equipment-title">Laboratory Equipment</h2>
        <div id="equipment-list"></div>
      </section>
      <section id="env-annotations" aria-labelledby="annotations-title">
        <h2 id="annotations-title">Annotated Objects</h2>
        <div id="annotations-list"></div>
      </section>
      <section id="env-legal" aria-labelledby="legal-title">
        <h2 id="legal-title">Legal & Compliance</h2>
        <div id="legal-list"></div>
      </section>
    </div>

    <script>
      // Source resolution
      const SOURCE_WIDTH = 1024;
      const SOURCE_HEIGHT = 506; 

      // Load and inject metadata
      async function loadMetadata() {
        try {
          // Load Reason
          const reasonRes = await fetch('reason.txt');
          if (reasonRes.ok) {
            const reasonText = await reasonRes.text();
            document.getElementById('reason-content').innerText = reasonText;
          }

          // Load Equipment
          const equipRes = await fetch('equipment.csv');
          if (equipRes.ok) {
            const csvText = await equipRes.text();
            const equipmentData = parseCSV(csvText);
            renderTable(equipmentData, 'equipment-list');
          }

          // Load Legal
          const legalRes = await fetch('legal.csv');
          if (legalRes.ok) {
            const legalText = await legalRes.text();
            const legalData = parseCSV(legalText);
            renderTable(legalData, 'legal-list');
          }

          // Load Annotations
          const annRes = await fetch('annotations.csv');
          if (annRes.ok) {
            const annText = await annRes.text();
            const annData = parseCSV(annText);
            renderAnnotations(annData, 'annotations-list');
            window.labAnnotations = annData; // Expose for Gaze Tracker
          }

        } catch (e) {
          console.error("Failed to load metadata:", e);
        }
      }

      function parseCSV(text) {
          // ... (existing helper) ...
        const lines = text.trim().split('\n');
        if (lines.length < 2) return []; // Access headers safely
        const headers = parseLine(lines[0]);
        
        return lines.slice(1).map(line => {
          const values = parseLine(line);
          return headers.reduce((obj, header, index) => {
            obj[header.trim()] = values[index];
            return obj;
          }, {});
        });
      }
      
      // ... (parseLine helper) ...
       function parseLine(line) {
        const result = [];
        let current = '';
        let inQuotes = false;
        
        for (let i = 0; i < line.length; i++) {
          const char = line[i];
          if (char === '"') {
            inQuotes = !inQuotes;
          } else if (char === ',' && !inQuotes) {
            result.push(current.trim());
            current = '';
          } else {
            current += char;
          }
        }
        result.push(current.trim());
        return result;
      }
      
      // ... (renderTable helper) ...
      function renderTable(data, containerId) {
        const container = document.getElementById(containerId);
        if (!container) return;
        
        const table = document.createElement('table');
        const thead = document.createElement('thead');
        const tbody = document.createElement('tbody');

        if (!data || data.length === 0) return;

        // Headers
        const headers = Object.keys(data[0]);
        const trHead = document.createElement('tr');
        headers.forEach(h => {
          const th = document.createElement('th');
          th.innerText = h;
          trHead.appendChild(th);
        });
        thead.appendChild(trHead);

        // Rows
        data.forEach(item => {
          const tr = document.createElement('tr');
          headers.forEach(h => {
            const td = document.createElement('td');
            td.innerText = item[h] || '';
            tr.appendChild(td);
          });
          tbody.appendChild(tr);
        });

        table.appendChild(thead);
        table.appendChild(tbody);
        container.appendChild(table);
      }

      function renderAnnotations(data, containerId) {
        const container = document.getElementById(containerId);
        if (!container || !data || data.length === 0) return;

        const uList = document.createElement('ul');
        
        data.forEach(item => {
          const x = parseFloat(item.x);
          const y = parseFloat(item.y);
          const w = parseFloat(item.width);
          const h = parseFloat(item.height);
          
          if (isNaN(x) || isNaN(y)) return;

          // Normalize to percentage for description
          const top = (y / SOURCE_HEIGHT * 100).toFixed(2);
          const left = (x / SOURCE_WIDTH * 100).toFixed(2);
          const width = (w / SOURCE_WIDTH * 100).toFixed(2);
          const height = (h / SOURCE_HEIGHT * 100).toFixed(2);

          const li = document.createElement('li');
          // Text description for AI agent
          li.innerText = `${item.annotation}: located at position top=${top}%, left=${left}%, width=${width}%, height=${height}% of the source view.`;
          uList.appendChild(li);
        });

        container.appendChild(uList);
      }

      // Initialize
      loadMetadata();
    </script>
  </body>
</html>
